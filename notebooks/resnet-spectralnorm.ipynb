{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfb = tfp.bijectors\n",
    "slim = tf.contrib.slim\n",
    "from tensorflow.python.framework import constant_op\n",
    "# from layers import wide_resnet\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow_probability\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow_hub as hub\n",
    "tfd = tensorflow_probability.distributions\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#\n",
    "import sys\n",
    "sys.path.append('../code/utils/')\n",
    "sys.path.append('../code')\n",
    "import tools\n",
    "from layers import wide_resnet\n",
    "#import datalib as dlib\n",
    "import datatools as dtools\n",
    "from time import time\n",
    "#\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.contrib.slim import add_arg_scope\n",
    "import tensorflow_probability\n",
    "tfd = tensorflow_probability.distributions\n",
    "\n",
    "# tf.enable_eager_execution()\n",
    "\n",
    "#############################\n",
    "seed_in = 3\n",
    "from numpy.random import seed\n",
    "seed(seed_in)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(seed_in)\n",
    "\n",
    "bs = 400\n",
    "nc, ncf = 128, 512\n",
    "ncp = 128\n",
    "step, stepf = 5, 40\n",
    "# path = '/data2/cosmo4d/'\n",
    "path = '../data/z00//'\n",
    "\n",
    "ftype = 'L%04d_N%04d_S%04d_%02dstep/'\n",
    "ftypefpm = 'L%04d_N%04d_S%04d_%02dstep_fpm/'\n",
    "numd = 1e-3\n",
    "num = int(numd*bs**3)\n",
    "R1 = 3\n",
    "R2 = 3*1.2\n",
    "kny = np.pi*ncp/bs\n",
    "kk = tools.fftk((ncp, ncp, ncp), bs)\n",
    "seeds = [100, 200, 300, 400]\n",
    "rprob = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cube_sizes = np.array([ 16, 32, 64, 128])\n",
    "# cube_sizes = np.array([64])\n",
    "num_cubes= (500*8/cube_sizes).astype('int')\n",
    "pad = 0\n",
    "cube_sizesft = cube_sizes + 2*pad\n",
    "max_offset = ncp - cube_sizes\n",
    "ftname = ['cic']\n",
    "tgname = ['pnn']\n",
    "nchannels = len(ftname)\n",
    "rprob = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the mesh have been generated for seed = 100\n",
      "For size =  16\n",
      "Length of targets =  1\n",
      "Supplemented by rotation :  0\n",
      "For size =  32\n",
      "Length of targets =  1\n",
      "Supplemented by rotation :  0\n",
      "For size =  64\n",
      "Length of targets =  1\n",
      "Supplemented by rotation :  0\n",
      "For size =  128\n",
      "All the mesh have been generated for seed = 200\n",
      "For size =  16\n",
      "Length of targets =  1\n",
      "Supplemented by rotation :  0\n",
      "For size =  32\n",
      "Length of targets =  1\n",
      "Supplemented by rotation :  0\n",
      "For size =  64\n",
      "Length of targets =  1\n",
      "Supplemented by rotation :  0\n",
      "For size =  128\n",
      "All the mesh have been generated for seed = 300\n",
      "For size =  16\n",
      "Length of targets =  1\n",
      "Supplemented by rotation :  0\n",
      "For size =  32\n",
      "Length of targets =  1\n",
      "Supplemented by rotation :  0\n",
      "For size =  64\n",
      "Length of targets =  1\n",
      "Supplemented by rotation :  0\n",
      "For size =  128\n",
      "All the mesh have been generated for seed = 400\n",
      "For size =  16\n",
      "Length of targets =  1\n",
      "Supplemented by rotation :  0\n",
      "For size =  32\n",
      "Length of targets =  1\n",
      "Supplemented by rotation :  0\n",
      "For size =  64\n",
      "Length of targets =  1\n",
      "Supplemented by rotation :  0\n",
      "For size =  128\n",
      "(248, 16, 16, 16, 1) (248, 16, 16, 16, 1)\n",
      "(60, 32, 32, 32, 1) (60, 32, 32, 32, 1)\n",
      "(12, 64, 64, 64, 1) (12, 64, 64, 64, 1)\n",
      "(4, 128, 128, 128, 1) (4, 128, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "#############################\n",
    "##Read data and generate meshes\n",
    "meshes = {}\n",
    "cube_features, cube_target = [[] for i in range(len(cube_sizes))], [[] for i in range(len(cube_sizes))]\n",
    "\n",
    "for seed in seeds:\n",
    "    mesh = {}\n",
    "    partp = tools.readbigfile(path + ftypefpm%(bs, nc, seed, step) + 'dynamic/1/Position/')\n",
    "    mesh['cic'] = tools.paintcic(partp, bs, ncp)\n",
    "    mesh['R1'] = tools.fingauss(mesh['cic'], kk, 10, kny)                                                                                                                                                                                                                                \n",
    "\n",
    "    hmesh = {}\n",
    "    ##Uncomment for galaxies\n",
    "#     hpath = path + ftype%(bs, ncf, seed, stepf) + 'galaxies_n05/galcat/'\n",
    "#     hposd = tools.readbigfile(hpath + 'Position/')\n",
    "#     massd = tools.readbigfile(hpath + 'Mass/').reshape(-1)*1e10\n",
    "#     galtype = tools.readbigfile(hpath + 'gal_type/').reshape(-1).astype(bool)\n",
    "#     hmesh['pnnsat'] = tools.paintnn(hposd[galtype], bs, ncp)\n",
    "#     hmesh['pnncen'] = tools.paintnn(hposd[~galtype], bs, ncp)\n",
    "#     hmesh['pnn'] = tools.paintnn(hposd, bs, ncp)\n",
    "#     targetmesh = [hmesh['pnncen'], hmesh['pnnsat']]\n",
    "    \n",
    "    hposall = tools.readbigfile(path + ftype%(bs, ncf, seed, stepf) + 'FOF/PeakPosition/')[1:]    \n",
    "    hposd = hposall[:num].copy()\n",
    "    hmesh['pnn'] = tools.paintnn(hposd, bs, ncp)\n",
    "    massall = tools.readbigfile(path + ftype%(bs, ncf, seed, stepf) + 'FOF/Mass/')[1:]    \n",
    "    massd = massall[:num].copy().reshape(-1)*1e10\n",
    "    hmesh['pnn'] = tools.paintnn(hposd, bs, ncp)\n",
    "    hmesh['mnn'] = tools.paintnn(hposd, bs, ncp, massd)\n",
    "    hmesh['lmnn'] = np.log(1+hmesh['mnn'])\n",
    "#     targetmesh = [hmesh[tgname[0]]\n",
    "    targetmesh = [hmesh[i].copy() for i in tgname]\n",
    "                  \n",
    "    meshes[seed] = [mesh, hmesh]\n",
    "\n",
    "    print('All the mesh have been generated for seed = %d'%seed)\n",
    "\n",
    "    #Create training voxels\n",
    "    ftlist = [mesh[i].copy() for i in ftname]\n",
    "    ftlistpad = [np.pad(i, pad, 'wrap') for i in ftlist]\n",
    "    ntarget = len(targetmesh)\n",
    "\n",
    "    for i, size in enumerate(cube_sizes):\n",
    "        print('For size = ', size)\n",
    "        if size==nc:\n",
    "            features = [np.stack(ftlistpad, axis=-1)]\n",
    "            target = [np.stack(targetmesh, axis=-1)]\n",
    "        else:\n",
    "            numcubes = int(num_cubes[i]/size*4)\n",
    "            features, target = dtools.randomvoxels(ftlistpad, targetmesh, numcubes, max_offset[i],\n",
    "                                            size, cube_sizesft[i], seed=seed, rprob=0)\n",
    "        cube_features[i] = cube_features[i] + features\n",
    "        cube_target[i] = cube_target[i] + target\n",
    "\n",
    "# #\n",
    "for i in range(cube_sizes.size):\n",
    "    cube_target[i] = np.stack(cube_target[i],axis=0)\n",
    "    cube_features[i] = np.stack(cube_features[i],axis=0)\n",
    "    print(cube_features[i].shape, cube_target[i].shape)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAD6CAYAAABwBTSmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAGcFJREFUeJzt3X+wXOV93/HP5/6+uhJI/JIRyAEjlxmcSSG9NdROlAQSBzt2MHWmNhNncOtUTidubeqpQ9JO3HbaGc80CUnHnriyQ/GMKXaCIWFcxzahTVVPgVhgHIPBRtiAhCQkfurHlXR/ffvHXbXLReLe79ndcx7dfb9mNPfu7vPc53t296vvnrPnPI8jQgAAoFkDTQcAAAAoyAAAFIGCDABAASjIAAAUgIIMAEABKMgAABSAgtzHbP+q7W80HQcAgILc1yLi1oh4W9NxACud7UNt/+ZtH2m7/as1xzJmO2yfv0S7620/3Yrxdtun1xVjv6IgA0CPRcTq4/8kPS3pXW333Zr5W7aHehPlK8a4TNJ/lvReSedKsqQ/6vW4/Y6C3Cdsb7R9h+39tp+3/SnbH7D9zbY2b7J9t+0XbD9r+3eajBnoF7bfavt+2y/Z3m37puOFt22P9p/ZfkLSw637f8n2460+f2j7Ptvvb/ubH7L9/VY+/3fb57Ue2tb6+f3W3u+7TxDSr0n6ckTcGxEHJf2upPfaHuvZkwAKcj+wPSjpK5KeknSBpPMkfXFRmzWS/krS1yRtkLRJ0j21Bgr0rxlJH5Z0pqSflvQuSb++qM07Jf09SZfZPlfSlyTdIOlsSbtbj0mSbL9X0kdbf2e9pG9L+kLr4c2tnxe39tD//ATxvEnSd47fiIhHJA1Juqj6JmIpFOT+8GYtFNl/FRGHI+JoRHxzUZt3StobEb/fevxgRNxff6hA/4mIv4mIb0XEXEQ8Ielzkn5mUbP/GBEvRcQRLRTab0XEVyJiRtLvSXqxre1vSPoPEfGD1uP/TtJP2V6/zJBWS3p50X0HJK1JbhoSKMj9YaOkpyJidok2T9QUD4A2ti+x/Zetr4oOaOEQ8VmLmu1s+31D++2ImJf0TNvjPybpM63D2S9J2i9pVtJrnsjV5pCk0xbdt0bSwWX2RwUU5P6wU9LrlzgZZKekN9QUD4BX+qykByVdFBGnSfr3WjiRql370nx71FZcbQ9o4auo43ZK+kBErG37Nx4RDyz6OyfziKS/2/b3L5E0Jz609xQFuT/8jRYS+JO2J1onibx1UZuvSDrX9kdtj9peY/vy+kMF+tIaSS9HxCHbb5L0T5dof5eky22/o/VB+19KWtf2+Gck/RvbF0uS7XW23yNJEXFMC4ejX+sD+Bckvcf25bZXa+GQ95ci4miVjcPyUJD7QETMaeE7p01auORilxYuZ2hvc1DSL7Ta7ZX0uKSfqzdSoG/dIOnXbR+S9GktnLB1UhGxR9J1Wrg06Tkt7C1/V9Kx1uO3SfqUpDtah8Af0kJ+H/e7kv6sdUj7l0/w9x/UwklhX5b0rBZqxUc62UAszRHLOXoBAChVay95rxaub7636XhQDXvIAHAKsv1226e3rg3+hKQpSQ80HBY6QEEGgFPTZkk/krRP0lWSro2I6WZDQic4ZA0AQAHYQwYAoAAUZAAACtDzVUPajQyOx/jQ4slfljA7l2s/nN+kGBpM93nVJftLNT9a4asd1/B5KbkdC33yccVgfqAYzo3jufzXLz6cv6yyjq95DurF5yLi7J4PVNGIR2NME02H8Srza/MxzSQngxzdeTg9xkrCc5yTyeVaC/L40Gl6y4bc0p/z+59PtfeG5U7V+v/NnpP8kCBJzhWYoceezg8xPp7ukzZYobiOjqT7zE/kF4k5cn4u8UcOzKTHGLr/0XSf+aO9nxvhr+L2p3o+SAfGNKHLfVXTYbzK1JX5uWx2b87l8qYb7kuPsZLwHOdkcplD1gAAFKCjgmz76tZ6mzts39itoADUj3wGmlW5ILfW2P20pLdLukTSda0JyAGcYshnoHmd7CG/WdKOiPhh62L0L0q6pjthAagZ+Qw0rJOCfJ5euT7nLr1y+S9Jku0ttrfb3j49d6SD4QD00JL53J7LMwtrGADoop6f1BURWyNiMiImRwZrOGsYQE+05/KwRpsOB1hxOinIz0ja2Hb7/NZ9AE495DPQsE4K8rckvdH2hbZHJL1PC4tmAzj1kM9AwypPDBIRs7Y/LOnrkgYl3RwRj3QtMgC1IZ+B5nU0U1dEfFXSV7sUC4AGkc9As5ipCwCAAtQ6l7UGBhQTuTOtBwbPSbWfqzDx+bEz82eMju9NXsI1M5seQ6sqLMhwJBnXGWvTY3gqP5fzYIU+o2O5t+f8WH6REF+4celGiww+91Kuw0yFhUVezHdBfs5kSdqwbWWsCT91bX6O6VV33l9Ln013pruk1bX9vcQeMgAABaAgAwBQAAoyAAAFoCADAFAACjIAAAWgIAMAUAAKMgAABaAgAwBQAAoyAAAFoCADAFAACjIAAAWodS7rGLDmx4dTfTycm5946rxVqfaSNHR4Lt1ncM8Lqfbzc/kxPDOT7hNTubmsB8bG0mNotsK83M7PMTx4+Fiq/dFzTkuPEc73GVqTe86OnJubv12S9Of5LqXLzjVc1zzDpc1nDGnHTVek+1SZk7y09yR7yAAAFICCDABAASoXZNsbbf9P29+z/Yjtj3QzMAD1IZ+B5nXyHfKspI9FxIO210h6wPbdEfG9LsUGoD7kM9CwynvIEbEnIh5s/X5Q0qOSzutWYADqQz4DzevKd8i2L5B0mSROVwROceQz0IyOC7Lt1ZK+LOmjEXHgBI9vsb3d9vaZ2alOhwPQQ6+Vz6/IZeUuSQOwtI4Ksu1hLSTvrRFxx4naRMTWiJiMiMnhofw1wgDqsVQ+vyKXNVp/gMAK18lZ1pb0J5IejYg/6F5IAOpGPgPN62QP+a2Sfk3SlbYfav17R5fiAlAv8hloWOXLniLim5Ly8yECKA75DDSPmboAAChArYtLzI0N6uW/sybVZ3hqPtV+5EB+4YORvQfTfRS5icw9mFskQ5LmD+TjiuQiFvMvvJgew6sqLJbg/Gc/T+XO5F396PPpMWJV/uSkY+fkTk7cf1mFNFuBi0ts+q3cHCO778yPUWWBgZViJS2SUerrWGXRC3309mU3ZQ8ZAIACUJABACgABRkAgAJQkAEAKAAFGQCAAlCQAQAoAAUZAIACUJABACgABRkAgAJQkAEAKAAFGQCAAlCQAQAoQK2LS8wPS1Ovy30GmB3Ltd/4tcOp9pLkl/KLOMw990Kuw0B+ZbuBVblFDCRJ09O59slFMiTJY2PpPjGeX8TB0zO5DhUW8NBMbjEOSYrka3nGI/kxSnds44R2fCw50f4V9/UmmDYraYGFUk1de3m6T/Z1KfV13KD8tj+ZaMseMgAABei4INsetP1t21/pRkAAmkM+A83pxh7yRyQ92oW/A6B55DPQkI4Ksu3zJf2SpM91JxwATSGfgWZ1uof8h5I+Lmm+C7EAaBb5DDSockG2/U5J+yLigSXabbG93fb2uan8GdAAem85+fyKXD5ELgPd1ske8lsl/bLtJyV9UdKVtr+wuFFEbI2IyYiYHFw10cFwAHpoyXx+RS6vJpeBbqtckCPityPi/Ii4QNL7JP2PiHh/1yIDUBvyGWge1yEDAFCArszUFRF/Lemvu/G3ADSLfAaawR4yAAAFqHUu66EjoTMezc1P/OSv5OZanjkjP8/y4N50F3k499R5zer8GM7Pf+2J3PzX82esSY8xszo/L/XQi1PpPjqam5c7JkbyY8zMprvMD+del1V7k/OLnwJGdx7Wphtyc1PvuCk39/WGbfl51kudA7mO+Z/rUiWuKtufVSWuOt6TGewhAwBQAAoyAAAFoCADAFAACjIAAAWgIAMAUAAKMgAABaAgAwBQAAoyAAAFoCADAFAACjIAAAWgIAMAUAAKMgAABah1cYmB6TmN7zyQ6nPO/zoj1d7zx1LtJSlW5xZkWBhnPtdhzUR6jPmJ/EIZM+vGU+0Pvj6/UMTgdH6C9dMP5F8XD+Y+L86dln++ZieG031mVuXiWvXUkfQYpZtfO6GpK3MLBmQXo6hiJS3iUIcN9+UXl9l9xcF0n5XyHPd6O9hDBgCgABRkAAAK0FFBtr3W9u22H7P9qO1/0K3AANSLfAaa1el3yH8k6WsR8Su2RyTlv4wFUAryGWhQ5YJs+3RJmyV9QJIiYlrSdHfCAlAn8hloXieHrC+UtF/Sf7X9bdufs/2qU4ltb7G93fb26bmpDoYD0ENL5nN7Ls8cO9RMlMAK1klBHpL0k5L+OCIuk3RY0o2LG0XE1oiYjIjJkUGOgAGFWjKf23N5eHR1EzECK1onBXmXpF0RcfzCrNu1kNAATj3kM9CwygU5IvZK2mn74tZdV0n6XleiAlAr8hloXqdnWf9zSbe2zsj8oaR/3HlIABpCPgMN6qggR8RDkia7FAuABpHPQLOYqQsAgALUurhEDFjzY7khp9Y7N8ZgfrGEtUfzE6wPRW6Bhfl1+bNSqyx8MDs+mGo/MJtfKOLw+vznuNnRdek+Zz6Ue+09m1zwQ9LLbxhJ9zl6Zi6u1U/n35MrUXYhg5W0iMHuzbn3jCRtujPXvsrCGruvKPP5qqLK9tcyxh23L7spe8gAABSAggwAQAEoyAAAFICCDABAASjIAAAUgIIMAEABKMgAABSAggwAQAEoyAAAFICCDABAASjIAAAUoNa5rOdHBjS1cSLV5/Dr51Lth47m5nKWpBjKfy7Zv/l1qfZHzsnPZTv2XH6e6bVPHEu1f/Hs/HzZc2PpLpr5hy+m+zxzbe45G70rPyf56Mv559i5t6SG9+S3vXQza/LzM+++902p9k/s/kyqvST99G9+KN2nyjzTdahjbuYqvr77oXSfi770G6n2G7bl87LKPObZ+cJ7jT1kAAAKQEEGAKAAHRVk2zfYfsT2w7Zvs13hYCaAEpDPQLMqF2Tb50n6F5ImI+LHJQ1Kel+3AgNQH/IZaF6nh6yHJI3bHpK0StLuzkMC0BDyGWhQ5YIcEc9I+j1JT0vaI+nliPhGtwIDUB/yGWheJ4es10m6RtKFkjZImrD9/hO022J7u+3ts8cOV48UQM8sJ5/bc3nuELkMdFsnh6x/XtKPImJ/RMxIukPSWxY3ioitETEZEZNDo7lrkAHUZsl8bs/lwdXkMtBtnRTkpyVdYXuVbUu6StKj3QkLQM3IZ6BhnXyHfL+k2yU9KOm7rb+1tUtxAagR+Qw0r6OpMyPiE5I+0aVYADSIfAaaxUxdAAAUoNbFJWLAml6d+wzwhjumc2MMzqbaS9LIk/vTfVadviHVfvz59BAaOpxcxaCCVfvn033WPHkk3WfmO/mFH3ZdmXt7nnYgvy2jL+TfL8OHk59jj+YW/DgVjO48rE033NfTMX7xhkvzna7Nd6mykEFWlYUPsotLVFokY/MV6S6/mPuvT5K04dreP8crAXvIAAAUgIIMAEABKMgAABSAggwAQAEoyAAAFICCDABAASjIAAAUgIIMAEABKMgAABSAggwAQAEoyAAAFICCDABAAWpdXMLzoZFDuQUAYiA3YfrwS0dT7SUpxkfTfSZ+kFstIsaG02PMnj6e7jN0MLeQwbqdL6bH0ED+c9zQvvzCDxceOC3dJ2vwcG7xEkn60XvWptqfNXZBegzdnu9SuuxiCVUWZKjSpw7ZbZfy27LpzvQQxaqyUEaV7d9xU25xjV4vqMIeMgAABViyINu+2fY+2w+33XeG7bttP976ua63YQLoBvIZKNdy9pBvkXT1ovtulHRPRLxR0j2t2wDKd4vIZ6BISxbkiNgm6YVFd18j6fOt3z8v6d1djgtAD5DPQLmqfoe8PiL2tH7fK2l9l+IBUD/yGShAxyd1RURIipM9bnuL7e22t88eO9zpcAB66LXyuT2XZ5Q7mx/A0qoW5GdtnytJrZ/7TtYwIrZGxGRETA6NTlQcDkAPLSuf23N5WPlLBQG8tqoF+S5J17d+v17SX3QnHAANIJ+BAiznsqfbJN0r6WLbu2x/UNInJf2C7ccl/XzrNoDCkc9AuZacqSsirjvJQ1d1ORYAPUY+A+Vipi4AAApAQQYAoAD1Li4RkmdzfQaPJjs4Pyl5lYUfPJW77GPgUH7Ri6EK2zKwL7lYxOBgegwN5982MZJ/joeeO5Rq7+mZ9BgxkV/AY9XuXPuXNlV4jleg7GIJdSzIUJdS4ypVrxdxqDpOlfek7lj+SjHsIQMAUAAKMgAABaAgAwBQAAoyAAAFoCADAFAACjIAAAWgIAMAUAAKMgAABaAgAwBQAAoyAAAFoCADAFCAeueynpeGD+Xmph48PJ1qP7dqJNVekoafP5juE1NHcu3XTKTH8NEKczNP554vj+fncq4yL7Xm5tJdPB+p9lFhjm1VmP96ILkpUxvm02NA2r05P5f7pjt7EEgXrKR5uVeS7Ouy6be+lx7j/9yx/LbsIQMAUAAKMgAABViyINu+2fY+2w+33fefbD9m+29t32l7bW/DBNAN5DNQruXsId8i6epF990t6ccj4ick/UDSb3c5LgC9cYvIZ6BISxbkiNgm6YVF930jIo6fnXWfpPN7EBuALiOfgXJ14zvkfyLpL0/2oO0ttrfb3j49fbgLwwHooZPmc3suz+hYzWEBK19HBdn2v5Y0K+nWk7WJiK0RMRkRkyMj+Ut/ANRjqXxuz+VhjdYbHNAHKl+HbPsDkt4p6aqIyF0wCqAo5DPQvEoF2fbVkj4u6WciYqq7IQGoE/kMlGE5lz3dJuleSRfb3mX7g5I+JWmNpLttP2T7Mz2OE0AXkM9AuZbcQ46I605w95/0IBYAPUY+A+Vipi4AAApQ6+ISs+PWcz8xlupzxmO5zwxDB/OLBWg+P/n//IazU+2nz8xttySNP7on3Sd9Nk6VRR8qLMgwP5FfxEIzuYVIYjT/dp5dl49rILd+hzSw8s6ROrZxQjs+dkWqz4Ztuedh0w33pdrXpd8Xiih1+3fclHs/Svn32LbN+TEy2EMGAKAAFGQAAApAQQYAoAAUZAAACkBBBgCgABRkAAAKQEEGAKAAFGQAAApAQQYAoAAUZAAACkBBBgCgABRkAAAKUOviEgPTodOeyi0YMLovt166Dx1JtZekGBtN9zm2flVuDKeHkAYqfF5ybqA4kn++XOH5qrKIw0ubcgtyTOzLvbck6ejawXSf/W/JjTO0psKCJyjWSloooordmyv8Z5ZclCG7EEnVPtkFKaqM8WSiLXvIAAAUYMmCbPtm2/tsP3yCxz5mO2yf1ZvwAHQT+QyUazl7yLdIunrxnbY3SnqbpKe7HBOA3rlF5DNQpCULckRsk/TCCR66SdLHJa281deBFYp8BspV6Ttk29dIeiYivtPleADUjHwGypA+y9r2Kkm/o4XDW8tpv0XSFkkaHV+bHQ5AD2XyuT2XB9et63FkQP+psod8kaQLJX3H9pOSzpf0oO3XnahxRGyNiMmImBwanageKYBeWHY+t+fy4GpyGei29B5yRHxX0jnHb7eSeDIinutiXABqQD4D5VjOZU+3SbpX0sW2d9n+YO/DAtAL5DNQriX3kCPiuiUev6Br0QDoKfIZKBczdQEAUIB657KeCY3tP5bq4+ncvMGenUu1lyQdys2XLUmrHptOtZ/ZkD8rNcbzc0bH/txXf65yck5yvmxJGjiWn2d6zTO5OaA9l7+E9rQn8/NM7//7uTm2H3/XZ9Nj5GfYrtfwwfy8vtk5oKeuvTzVvsoYVcfJWknzX2+64b50n+xzXGW+7CpxbVAurv/96f+SHmPwjuW3ZQ8ZAIACUJABACgABRkAgAJQkAEAKAAFGQCAAlCQAQAoAAUZAIACUJABACgABRkAgAJQkAEAKAAFGQCAAlCQAQAogCPyE/JXHszeL+mpEzx0lqR+XhC9n7e/n7ddOvn2/1hEnF13MMv1Grks9fdr2s/bLvX39necy7UW5JMGYW+PiMmm42hKP29/P2+7tDK3fyVu03L187ZL/b393dh2DlkDAFAACjIAAAUopSBvbTqAhvXz9vfztksrc/tX4jYtVz9vu9Tf29/xthfxHTIAAP2ulD1kAAD6WuMF2fbVtr9ve4ftG5uOp062n7T9XdsP2d7edDy9Zvtm2/tsP9x23xm277b9eOvnuiZj7JWTbPu/tf1M6/V/yPY7moyxU/2cy1J/5XM/57LUu3xutCDbHpT0aUlvl3SJpOtsX9JkTA34uYi4tE8uFbhF0tWL7rtR0j0R8UZJ97Rur0S36NXbLkk3tV7/SyPiqzXH1DXk8v/TL/l8i/o3l6Ue5XPTe8hvlrQjIn4YEdOSvijpmoZjQo9ExDZJLyy6+xpJn2/9/nlJ7641qJqcZNtXEnK5j/RzLku9y+emC/J5kna23d7Vuq9fhKRv2H7A9pamg2nI+ojY0/p9r6T1TQbTgA/b/tvWIbBT+RBfv+eyRD73ey5LHeZz0wW53/1URPykFg7z/abtzU0H1KRYOOW/n077/2NJF0m6VNIeSb/fbDjoEPnc0oe5LHUhn5suyM9I2th2+/zWfX0hIp5p/dwn6U4tHPbrN8/aPleSWj/3NRxPbSLi2YiYi4h5SZ/Vqf3693UuS+Sz+jiXpe7kc9MF+VuS3mj7Qtsjkt4n6a6GY6qF7Qnba47/Lultkh5+7V4r0l2Srm/9fr2kv2gwllod/8+r5Vqd2q9/3+ayRD639G0uS93J56HuhZMXEbO2Pyzp65IGJd0cEY80GVON1ku607a08Dr8t4j4WrMh9Zbt2yT9rKSzbO+S9AlJn5T0p7Y/qIXVg/5RcxH2zkm2/WdtX6qFQ3tPSvpQYwF2qM9zWeqzfO7nXJZ6l8/M1AUAQAGaPmQNAABEQQYAoAgUZAAACkBBBgCgABRkAAAKQEEGAKAAFGQAAApAQQYAoAD/FwI1esWvrxFIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, nchannels+ntarget, figsize = (nchannels*4+4, 5))\n",
    "n = 10\n",
    "for i in range(nchannels):\n",
    "    ax[i].imshow(cube_features[0][n][:,:,:,i].sum(axis=0))\n",
    "    ax[i].set_title(ftname[i])\n",
    "for j in range(ntarget):\n",
    "    ax[i+j+1].imshow(cube_target[0][n][:,:,:,j].sum(axis=0))\n",
    "    ax[i+j+1].set_title('Target %d'%j)\n",
    "# ax[-1].imshow(cube_target[n][:,:,:,0].sum(axis=0))\n",
    "# ax[-1].set_title('Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cube_target[0].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "\n",
    "# NO_OPS = 'NO_OPS'\n",
    "\n",
    "# def _l2normalize(v, eps=1e-12):\n",
    "#     return v / (tf.reduce_sum(v ** 2) ** 0.5 + eps)\n",
    "\n",
    "\n",
    "# def spectral_normed_weight(W, u=None, num_iters=1, update_collection=None, with_sigma=False):\n",
    "#   # Usually num_iters = 1 will be enough\n",
    "#     W_shape = W.shape.as_list()\n",
    "#     W_reshaped = tf.reshape(W, [-1, W_shape[-1]])\n",
    "#     if u is None:\n",
    "#         u = tf.get_variable(\"u\", [1, W_shape[-1]], initializer=tf.truncated_normal_initializer(), trainable=False)\n",
    "#     def power_iteration(i, u_i, v_i):\n",
    "#         v_ip1 = _l2normalize(tf.matmul(u_i, tf.transpose(W_reshaped)))\n",
    "#         u_ip1 = _l2normalize(tf.matmul(v_ip1, W_reshaped))\n",
    "#         return i + 1, u_ip1, v_ip1\n",
    "    \n",
    "#     _, u_final, v_final = tf.while_loop(\n",
    "#         cond=lambda i, _1, _2: i < num_iters,\n",
    "#         body=power_iteration,\n",
    "#         loop_vars=(tf.constant(0, dtype=tf.int32),\n",
    "#                    u, tf.zeros(dtype=tf.float32, shape=[1, W_reshaped.shape.as_list()[0]]))\n",
    "#       )\n",
    "    \n",
    "#     if update_collection is None:\n",
    "#         warnings.warn('Setting update_collection to None will make u being updated every W execution. This maybe undesirable'\n",
    "#                   '. Please consider using a update collection instead.')\n",
    "#         sigma = tf.matmul(tf.matmul(v_final, W_reshaped), tf.transpose(u_final))[0, 0]\n",
    "#         # sigma = tf.reduce_sum(tf.matmul(u_final, tf.transpose(W_reshaped)) * v_final)\n",
    "#         W_bar = W_reshaped / sigma\n",
    "#         with tf.control_dependencies([u.assign(u_final)]):\n",
    "#             W_bar = tf.reshape(W_bar, W_shape)\n",
    "#     else:\n",
    "#         sigma = tf.matmul(tf.matmul(v_final, W_reshaped), tf.transpose(u_final))[0, 0]\n",
    "#         # sigma = tf.reduce_sum(tf.matmul(u_final, tf.transpose(W_reshaped)) * v_final)\n",
    "#         W_bar = W_reshaped / sigma\n",
    "#         W_bar = tf.reshape(W_bar, W_shape)\n",
    "#         # Put NO_OPS to not update any collection. This is useful for the second call of discriminator if the update_op\n",
    "#         # has already been collected on the first call.\n",
    "#         if update_collection != NO_OPS:\n",
    "#             tf.add_to_collection(update_collection, u.assign(u_final))\n",
    "#     if with_sigma:\n",
    "#         return W_bar, sigma\n",
    "#     else:\n",
    "#         return W_bar\n",
    "\n",
    "\n",
    "###################################################\n",
    "\n",
    "# @add_arg_scope\n",
    "# def scope_has_variables(scope):\n",
    "#     return len(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=scope.name)) > 0\n",
    "\n",
    "# @add_arg_scope\n",
    "# def myconv3d(input_, output_dim,\n",
    "#            kernel_size=3, stride=1, stddev=None,\n",
    "#            name=\"conv3d\", spectral_normed=True, update_collection=None, with_w=False, \n",
    "#              padding=\"SAME\", reuse=tf.AUTO_REUSE):\n",
    "\n",
    "#     k_h, k_w, k_z = [kernel_size]*3\n",
    "#     d_h, d_w, d_z = [stride]*3 \n",
    "#     # Glorot intialization\n",
    "#   # For RELU nonlinearity, it's sqrt(2./(n_in)) instead\n",
    "#     fan_in = k_h * k_w * k_z * input_.get_shape().as_list()[-1]\n",
    "#     fan_out = k_h * k_w * k_z * output_dim\n",
    "#     if stddev is None:\n",
    "#         stddev = np.sqrt(2. / (fan_in))\n",
    "\n",
    "#     with tf.variable_scope(name) as scope:\n",
    "#         if scope_has_variables(scope):\n",
    "#             scope.reuse_variables()\n",
    "#         w = tf.get_variable(\"w\", [k_h, k_w, k_z, input_.get_shape()[-1], output_dim],\n",
    "#                             initializer=tf.truncated_normal_initializer(stddev=stddev))\n",
    "#         if spectral_normed:\n",
    "#             conv = tf.nn.conv3d(input_, spectral_normed_weight(w, update_collection=update_collection),\n",
    "#                               strides=[1, d_h, d_w, d_z, 1], padding=padding)\n",
    "#         else:\n",
    "#             conv = tf.nn.conv3d(input_, w, strides=[1, d_h, d_w, d_z, 1], padding=padding)\n",
    "\n",
    "#         biases = tf.get_variable(\"b\", [output_dim], initializer=tf.constant_initializer(0.0))\n",
    "#         conv = tf.nn.bias_add(conv, biases)\n",
    "#         if with_w:\n",
    "#             return conv, w, biases\n",
    "#         else:\n",
    "#             return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.contrib.framework.python.ops import add_arg_scope, arg_scope\n",
    "\n",
    "# # def int_shape(x):\n",
    "# #     print(x.get_shape())\n",
    "# #     if str(x.get_shape()[0]) != '?':\n",
    "# #         return list(map(int, x.get_shape()))\n",
    "# #     return [-1]+list(map(int, x.get_shape()[1:]))\n",
    "\n",
    "\n",
    "# @add_arg_scope\n",
    "# def get_variable_ddi(name, shape, initial_value, dtype=tf.float32, init=False, trainable=True):\n",
    "#     w = tf.get_variable(name, shape, dtype, None, trainable=trainable)\n",
    "#     if init:\n",
    "#         w = w.assign(initial_value)\n",
    "#         with tf.control_dependencies([w]):\n",
    "#             return w\n",
    "#     return w\n",
    "\n",
    "# @add_arg_scope\n",
    "# # def actnorm3d(name, x, scale=1., logdet=None, logscale_factor=3., batch_variance=False, \n",
    "# #             reverse=False, init=False, trainable=True):\n",
    "# #     if arg_scope([get_variable_ddi], trainable=trainable):\n",
    "# def actnorm3d(x, scale=1., logdet=None, logscale_factor=3., batch_variance=False, \n",
    "#             reverse=False, init=False, is_training=True, scope='actnorm'):\n",
    "#     name = scope\n",
    "#     if arg_scope([get_variable_ddi], trainable=is_training):\n",
    "#         if not reverse:\n",
    "#             x = actnorm_center3d(name+\"_center\", x, reverse)\n",
    "#             x = actnorm_scale3d(name+\"_scale\", x, scale, logdet,\n",
    "#                               logscale_factor, batch_variance, reverse, init)\n",
    "#             if logdet != None:\n",
    "#                 x, logdet = x\n",
    "#         else:\n",
    "#             x = actnorm_scale3d(name + \"_scale\", x, scale, logdet,\n",
    "#                               logscale_factor, batch_variance, reverse, init)\n",
    "#             if logdet != None:\n",
    "#                 x, logdet = x\n",
    "#             x = actnorm_center3d(name+\"_center\", x, reverse)\n",
    "#         if logdet != None:\n",
    "#             return x, logdet\n",
    "#         return x\n",
    "\n",
    "# # Activation normalization\n",
    "\n",
    "\n",
    "# @add_arg_scope\n",
    "# def actnorm_center3d(name, x, reverse=False):\n",
    "#     shape = x.get_shape()\n",
    "#     with tf.variable_scope(name):\n",
    "#         assert len(shape) == 5\n",
    "#         x_mean = tf.reduce_mean(x, [0, 1, 2, 3], keepdims=True)\n",
    "#         b = get_variable_ddi(\n",
    "# #             \"b\", (1, 1, 1, 1, int_shape(x)[4]), initial_value=-x_mean)\n",
    "#             \"b\", (1, 1, 1, 1, shape[4]), initial_value=-x_mean)\n",
    "\n",
    "#         if not reverse:\n",
    "#             x += b\n",
    "#         else:\n",
    "#             x -= b\n",
    "\n",
    "#         return x\n",
    "\n",
    "# # Activation normalization\n",
    "\n",
    "\n",
    "# @add_arg_scope\n",
    "# def actnorm_scale3d(name, x, scale=1., logdet=None, logscale_factor=3., \n",
    "#                   batch_variance=False, reverse=False, init=False, trainable=True):\n",
    "#     shape = x.get_shape()\n",
    "#     with tf.variable_scope(name), arg_scope([get_variable_ddi], trainable=trainable):\n",
    "#         assert len(shape) == 5\n",
    "#         x_var = tf.reduce_mean(x**2, [0, 1, 2, 3], keepdims=True)\n",
    "# #         logdet_factor = int(shape[1])*int(shape[2])*int(shape[3])\n",
    "#         logdet_factor = (shape[1])*(shape[2])*(shape[3])\n",
    "# #         _shape = (1, 1, 1, 1, int_shape(x)[4])\n",
    "#         _shape = (1, 1, 1, 1, shape[4])\n",
    "\n",
    "#         if batch_variance:\n",
    "#             x_var = tf.reduce_mean(x**2, keepdims=True)\n",
    "\n",
    "#         if True:\n",
    "#             logs = get_variable_ddi(\"logs\", _shape, initial_value=tf.log(\n",
    "#                 scale/(tf.sqrt(x_var)+1e-6))/logscale_factor)*logscale_factor\n",
    "#             if not reverse:\n",
    "#                 x = x * tf.exp(logs)\n",
    "#             else:\n",
    "#                 x = x * tf.exp(-logs)\n",
    "#         else:\n",
    "#             # Alternative, doesn't seem to do significantly worse or better than the logarithmic version above\n",
    "#             s = get_variable_ddi(\"s\", _shape, initial_value=scale /\n",
    "#                                  (tf.sqrt(x_var) + 1e-6) / logscale_factor)*logscale_factor\n",
    "#             logs = tf.log(tf.abs(s))\n",
    "#             if not reverse:\n",
    "#                 x *= s\n",
    "#             else:\n",
    "#                 x /= s\n",
    "\n",
    "#         if logdet != None:\n",
    "#             dlogdet = tf.reduce_sum(logs) * logdet_factor\n",
    "#             if reverse:\n",
    "#                 dlogdet *= -1\n",
    "#             return x, logdet + dlogdet\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @slim.add_arg_scope\n",
    "# def wide_resnet_snorm(inputs, depth, \n",
    "#                 keep_prob=None,\n",
    "#                 kernel_size=3, stride=1,\n",
    "#                 activation_fn=tf.nn.relu,\n",
    "#                 is_training=True,\n",
    "#                 outputs_collections=None, scope=None):\n",
    "#     \"\"\"\n",
    "#     Wide residual units as advocated in arXiv:1605.07146\n",
    "#     Adapted from slim implementation of residual networks\n",
    "#     Resample can be 'up', 'down', 'none' - removed for now\n",
    "#     \"\"\"\n",
    "#     depth_residual = 2 * depth\n",
    "\n",
    "#     depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)\n",
    "#     size_in = inputs.get_shape().as_list()[1]\n",
    "\n",
    "#     with tf.variable_scope(scope, 'wide_resnet', [inputs]) as sc:\n",
    "#         with slim.arg_scope([actnorm3d, slim.dropout],\n",
    "#                             is_training=is_training):\n",
    "#             with slim.arg_scope([myconv3d],\n",
    "#                                 kernel_size=kernel_size,\n",
    "#                                 stride=stride):\n",
    "\n",
    "#                 preact = actnorm3d(inputs, scope='preact')\n",
    "#                 preact = activation_fn(preact)\n",
    "                \n",
    "#                 if depth_in != depth:\n",
    "#                     shortcut = myconv3d(preact, depth, name='shortcut')\n",
    "#                 else:\n",
    "#                     shortcut = preact\n",
    "\n",
    "#                 residual = myconv3d(preact, depth_residual, name='res1')\n",
    "#                 #residual = myconv3d(preact, depth_residual, scope='res1')\n",
    "\n",
    "#                 if keep_prob is not None:\n",
    "#                     residual = slim.dropout(residual, keep_prob=keep_prob)\n",
    "\n",
    "#                 residual = myconv3d(residual, depth, name='res2')\n",
    "# #                 residual = myconv3d(residual, depth, stride=1, scope='res2',\n",
    "# #                                        normalizer_fn=None, activation_fn=None)\n",
    "\n",
    "#                 output = shortcut + residual\n",
    "\n",
    "#                 return slim.utils.collect_named_outputs(outputs_collections,\n",
    "#                                                         sc.name,\n",
    "#                                                         output)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import  wide_resnet_snorm\n",
    "from tfops import specnormconv3d\n",
    "\n",
    "\n",
    "def _mdn_model_fn(features, labels, n_y, n_mixture, dropout, optimizer, mode):\n",
    "\n",
    "    # Check for training mode\n",
    "    is_training = mode == tf.estimator.ModeKeys.TRAIN\n",
    "        \n",
    "    def _module_fn():\n",
    "        \"\"\"\n",
    "        Function building the module\n",
    "        \"\"\"\n",
    "    \n",
    "        feature_layer = tf.placeholder(tf.float32, shape=[None, None, None, None, nchannels], name='input')\n",
    "        obs_layer = tf.placeholder(tf.float32, shape=[None, None, None, None, n_y], name='observations')\n",
    "\n",
    "        # Builds the neural network\n",
    "        net = wide_resnet_snorm(feature_layer, 8, activation_fn=tf.nn.leaky_relu, is_training=is_training)\n",
    "        net = wide_resnet_snorm(net, 16, activation_fn=tf.nn.leaky_relu, keep_prob=dropout, is_training=is_training)\n",
    "        net = wide_resnet_snorm(net, 32, activation_fn=tf.nn.leaky_relu, keep_prob=dropout, is_training=is_training)\n",
    "        net = wide_resnet_snorm(net, 32, activation_fn=tf.nn.leaky_relu, keep_prob=dropout, is_training=is_training)\n",
    "        net = wide_resnet_snorm(net, 16, activation_fn=tf.nn.tanh)\n",
    "        \n",
    "        # Define the probabilistic layer \n",
    "        net = specnormconv3d(net, n_mixture*3*n_y, 1)\n",
    "\n",
    "        cube_size = tf.shape(obs_layer)[1]\n",
    "        net = tf.reshape(net, [-1, cube_size, cube_size, cube_size, n_y, n_mixture*3])\n",
    "#         net = tf.reshape(net, [None, None, None, None, n_y, n_mixture*3])\n",
    "        loc, unconstrained_scale, logits = tf.split(net,\n",
    "                                                    num_or_size_splits=3,\n",
    "                                                    axis=-1)\n",
    "        scale = tf.nn.softplus(unconstrained_scale)\n",
    "\n",
    "        # Form mixture of discretized logistic distributions. Note we shift the\n",
    "        # logistic distribution by -0.5. This lets the quantization capture \"rounding\"\n",
    "        # intervals, `(x-0.5, x+0.5]`, and not \"ceiling\" intervals, `(x-1, x]`.\n",
    "        discretized_logistic_dist = tfd.QuantizedDistribution(\n",
    "            distribution=tfd.TransformedDistribution(\n",
    "                distribution=tfd.Logistic(loc=loc, scale=scale),\n",
    "                bijector=tfb.AffineScalar(shift=-0.5)),\n",
    "            low=0.,\n",
    "            high=2**4 - 1.)\n",
    "\n",
    "        mixture_dist = tfd.MixtureSameFamily(\n",
    "            mixture_distribution=tfd.Categorical(logits=logits),\n",
    "            components_distribution=discretized_logistic_dist)\n",
    "\n",
    "        # Define a function for sampling, and a function for estimating the log likelihood\n",
    "        sample = tf.squeeze(mixture_dist.sample())\n",
    "        loglik = mixture_dist.log_prob(obs_layer)\n",
    "        hub.add_signature(inputs={'features':feature_layer, 'labels':obs_layer},\n",
    "                          outputs={'sample':sample, 'loglikelihood':loglik,\n",
    "                                   'loc':loc, 'scale':scale, 'logits':logits})\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # Create model and register module if necessary\n",
    "    spec = hub.create_module_spec(_module_fn)\n",
    "    module = hub.Module(spec, trainable=True)\n",
    "    if isinstance(features,dict):\n",
    "        predictions = module(features, as_dict=True)\n",
    "    else:\n",
    "        predictions = module({'features':features, 'labels':labels}, as_dict=True)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:    \n",
    "        hub.register_module_for_export(module, \"likelihood\")\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "    \n",
    "    loglik = predictions['loglikelihood']\n",
    "    # Compute and register loss function\n",
    "    neg_log_likelihood = -tf.reduce_sum(loglik, axis=-1)\n",
    "    neg_log_likelihood = tf.reduce_mean(neg_log_likelihood)\n",
    "    \n",
    "    tf.losses.add_loss(neg_log_likelihood)\n",
    "    total_loss = tf.losses.get_total_loss(add_regularization_losses=True)\n",
    "\n",
    "    train_op = None\n",
    "    eval_metric_ops = None\n",
    "\n",
    "    # Define optimizer\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            global_step=tf.train.get_global_step()\n",
    "            boundaries = [15000, 30000, 45000, 60000]\n",
    "            values = [0.001, 0.0005, 0.0001, 0.00005, 0.00001]\n",
    "            learning_rate = tf.train.piecewise_constant(global_step, boundaries, values)\n",
    "            train_op = optimizer(learning_rate=learning_rate).minimize(loss=total_loss, global_step=global_step)\n",
    "                                        \n",
    "        tf.summary.scalar('loss', neg_log_likelihood)\n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "        \n",
    "        eval_metric_ops = { \"log_p\": neg_log_likelihood}\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                      predictions=predictions,\n",
    "                                      loss=total_loss,\n",
    "                                      train_op=train_op,\n",
    "                                      eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "\n",
    "\n",
    "# def _mdn_model_fn(features, labels, n_y, n_mixture, dropout, optimizer, mode):\n",
    "\n",
    "#     # Check for training mode\n",
    "#     is_training = mode == tf.estimator.ModeKeys.TRAIN\n",
    "        \n",
    "#     def _module_fn():\n",
    "#         \"\"\"\n",
    "#         Function building the module\n",
    "#         \"\"\"\n",
    "    \n",
    "#         feature_layer = tf.placeholder(tf.float32, shape=[None, None, None, None, nchannels], name='input')\n",
    "#         obs_layer = tf.placeholder(tf.float32, shape=[None, None, None, None, n_y], name='observations')\n",
    "\n",
    "#         # Builds the neural network\n",
    "#         net = wide_resnet_snorm(feature_layer, 8, activation_fn=tf.nn.leaky_relu, is_training=is_training)\n",
    "#         net = wide_resnet_snorm(net, 16, activation_fn=tf.nn.leaky_relu, keep_prob=dropout, is_training=is_training)\n",
    "#         net = wide_resnet_snorm(net, 32, activation_fn=tf.nn.leaky_relu, keep_prob=dropout, is_training=is_training)\n",
    "# #         net = wide_resnet_snorm(net, 32, activation_fn=tf.nn.leaky_relu, keep_prob=dropout, is_training=is_training)\n",
    "#         net = wide_resnet_snorm(net, 16, activation_fn=tf.nn.leaky_relu)\n",
    "#         net = wide_resnet_snorm(net, 1, activation_fn=None)\n",
    "        \n",
    "#         # Define the probabilistic layer \n",
    "#         lbda = tf.nn.softplus(net, name='lambda') + 1e-5\n",
    "\n",
    "#         dist = tfd.Poisson(lbda)\n",
    "        \n",
    "#         sample = tf.squeeze(dist.sample())\n",
    "#         loglik = dist.log_prob(obs_layer)\n",
    "#         difference = (tf.subtract(obs_layer, net))\n",
    "    \n",
    "#         hub.add_signature(inputs={'features':feature_layer, 'labels':obs_layer}, \n",
    "#                           outputs={'sample':sample, 'loglikelihood':loglik, 'lambda':lbda, \n",
    "#                                    'difference':difference})\n",
    "    \n",
    "#     # Create model and register module if necessary\n",
    "#     spec = hub.create_module_spec(_module_fn)\n",
    "#     module = hub.Module(spec, trainable=True)\n",
    "#     if isinstance(features,dict):\n",
    "#         predictions = module(features, as_dict=True)\n",
    "#     else:\n",
    "#         predictions = module({'features':features, 'labels':labels}, as_dict=True)\n",
    "    \n",
    "#     if mode == tf.estimator.ModeKeys.PREDICT:    \n",
    "#         hub.register_module_for_export(module, \"likelihood\")\n",
    "#         return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "    \n",
    "#     # Compute and register loss function\n",
    "#     loglik = -predictions['loglikelihood']\n",
    "# #     diff = predictions['difference']\n",
    "# #     loglik = tf.square(diff)\n",
    "    \n",
    "#     neg_log_likelihood = tf.reduce_sum(loglik, axis=-1)\n",
    "#     neg_log_likelihood = tf.reduce_mean(neg_log_likelihood)\n",
    "#     tf.losses.add_loss(neg_log_likelihood)\n",
    "#     total_loss = tf.losses.get_total_loss(add_regularization_losses=True)\n",
    "\n",
    "#     train_op = None\n",
    "#     eval_metric_ops = None\n",
    "\n",
    "#     # Define optimizer\n",
    "#     if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "#         update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "#         with tf.control_dependencies(update_ops):\n",
    "#             global_step=tf.train.get_global_step()\n",
    "#             boundaries = [15000, 30000, 45000, 60000]\n",
    "#             values = [0.001, 0.0005, 0.0001, 0.00005, 0.00001]\n",
    "#             learning_rate = tf.train.piecewise_constant(global_step, boundaries, values)\n",
    "#             train_op = optimizer(learning_rate=learning_rate).minimize(loss=total_loss, global_step=global_step)\n",
    "                                        \n",
    "#         tf.summary.scalar('loss', neg_log_likelihood)\n",
    "#     elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "        \n",
    "#         eval_metric_ops = { \"log_p\": neg_log_likelihood}\n",
    "\n",
    "#     return tf.estimator.EstimatorSpec(mode=mode,\n",
    "#                                       predictions=predictions,\n",
    "#                                       loss=total_loss,\n",
    "#                                       train_op=train_op,\n",
    "#                                       eval_metric_ops=eval_metric_ops)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MDNEstimator(tf.estimator.Estimator):\n",
    "    \"\"\"An estimator for distribution estimation using Mixture Density Networks.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_y,\n",
    "                 nchannels,\n",
    "#                  cube_size,\n",
    "                 optimizer=tf.train.AdamOptimizer,\n",
    "                 dropout=None,\n",
    "                 model_dir=None,\n",
    "                 config=None):\n",
    "        \"\"\"Initializes a `MDNEstimator` instance.\n",
    "        \"\"\"\n",
    "        def _model_fn(features, labels, mode):\n",
    "            return _mdn_model_fn(features, labels, \n",
    "                 n_y, nchannels, #cube_size, \n",
    "                        dropout, optimizer, mode)\n",
    "\n",
    "        super(self.__class__, self).__init__(model_fn=_model_fn,\n",
    "                                             model_dir=model_dir,\n",
    "                                             config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "def mapping_function(inds):\n",
    "    def extract_batch(inds):\n",
    "\n",
    "        isize = np.random.choice(len(cube_sizes), 1, replace=True)[0]\n",
    "        batch = int(batch_size*8/cube_sizes[isize])\n",
    "        if cube_sizes[isize]==nc : batch = 1\n",
    "        inds = inds[:batch]\n",
    "        trainingsize = cube_features[isize].shape[0]\n",
    "        inds[inds >= trainingsize] =  (inds[inds >= trainingsize])%trainingsize\n",
    "\n",
    "        features = cube_features[isize][inds].astype('float32')\n",
    "        targets = cube_target[isize][inds].astype('float32')\n",
    "\n",
    "        for i in range(batch):\n",
    "            nrotations=0\n",
    "            while (np.random.random() < rprob) & (nrotations < 3):\n",
    "                nrot, ax0, ax1 = np.random.randint(0, 3), *np.random.permutation((0, 1, 2))[:2]\n",
    "                features[i] = np.rot90(features[i], nrot, (ax0, ax1))\n",
    "                targets[i] = np.rot90(targets[i], nrot, (ax0, ax1))\n",
    "                nrotations +=1\n",
    "#             print(batch, isize, i, nrotations, targets[i].shape, targets.shape)                                                                                                                   \n",
    "# #         print(inds)                                                                                                                                                         \n",
    "        return features, targets\n",
    "\n",
    "    ft, tg = tf.py_func(extract_batch, [inds],\n",
    "                        [tf.float32, tf.float32])\n",
    "#     sft = cube_features[isize].shape                                                                                                                                          \n",
    "#     stg = cube_target[isize].shape                                                                                                                                            \n",
    "#     ft.set_shape((None,)+sft[1:])                                                                                                                                             \n",
    "#     tg.set_shape((None,)+stg[1:])                                                                                                                                             \n",
    "    return ft, tg\n",
    "\n",
    "\n",
    "def training_input_fn():\n",
    "    \"\"\"Serving input fn for training data\"\"\"\n",
    "\n",
    "    dataset = tf.data.Dataset.range(len(np.array(cube_features)[0]))\n",
    "    dataset = dataset.repeat().shuffle(1000).batch(batch_size)\n",
    "    dataset = dataset.map(mapping_function)\n",
    "    dataset = dataset.prefetch(16)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def testing_input_fn():\n",
    "    \"\"\"Serving input fn for testing data\"\"\"\n",
    "    dataset = tf.data.Dataset.range(len(cube_features))\n",
    "    dataset = dataset.batch(16)\n",
    "    dataset = dataset.map(mapping_function)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': './models/n10/res-SN2///model/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f727a8acbe0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "savepath = './models/n10/res-SN2//'\n",
    "\n",
    "run_config = tf.estimator.RunConfig(save_checkpoints_steps = 1000)\n",
    "\n",
    "model =  MDNEstimator(n_y=ntarget, nchannels=nchannels, dropout=0.95,\n",
    "                      model_dir=savepath + '/model/', config = run_config)\n",
    "#                       model_dir='./tmp/galmodel/model0', config = run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to export\n",
    "features = tf.placeholder(tf.float32, shape=[None, None, None, None, nchannels], name='input')\n",
    "labels = tf.placeholder(tf.float32, shape=[None, None, None, None, ntarget], name='observations')\n",
    "    \n",
    "exporter = hub.LatestModuleExporter(\"tf_hub\",\n",
    "        tf.estimator.export.build_raw_serving_input_receiver_fn({'features':features, 'labels':labels},\n",
    "                                                                   default_batch_size=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "# ls ./models/n10/res-SNpois//\n",
    "# rm -r ./models/n10/res-SN2/\n",
    "# tf.summary.FileWriterCache.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ./models/n10/res-SN2///model/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.9378551, step = 0\n",
      "INFO:tensorflow:global_step/sec: 1.45901\n",
      "INFO:tensorflow:loss = 0.10256158, step = 100 (68.542 sec)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "model.train(training_input_fn, max_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "modulepath = exporter.export(model, savepath + '/module/', model.latest_checkpoint())\n",
    "modulepath = modulepath.decode(\"utf-8\")\n",
    "modulepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# modulepath = '/home/chmodi/Projects/galmodel/notebooks/models/n10/rev-SN/module/1548733709/'\n",
    "module = hub.Module(modulepath + '/likelihood/')\n",
    "\n",
    "# module_inf = hub.Module(modulepath + '/inference/')\n",
    "# module = hub.Module('../code/models/galmodel/pad2/module/1546484684/likelihood/')\n",
    "\n",
    "xx = tf.placeholder(tf.float32, shape=[None, None, None, None, nchannels], name='input')\n",
    "yy = tf.placeholder(tf.float32, shape=[None, None, None, None, ntarget], name='labels')\n",
    "samples = module(dict(features=xx, labels=yy), as_dict=True)['sample']\n",
    "# loglik = module(dict(features=xx, labels=yy), as_dict=True)['loglikelihood']\n",
    "# poisson = module(dict(features=xx, labels=yy), as_dict=True)['lambda']\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initializers.global_variables())\n",
    "        \n",
    "    features = cube_features[0][5:8].astype('float32')\n",
    "    targets = cube_target[0][5:8].astype('float32')\n",
    "    xxm = features\n",
    "    yym = targets\n",
    "    print(xxm.shape, yym.shape)\n",
    "    preds = sess.run(samples, feed_dict={xx:xxm, yy:yym})\n",
    "#     pois = sess.run(poisson, feed_dict={xx:xxm, yy:yym})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = imshow(preds[0,:,:,:].sum(axis=-1))\n",
    "plt.colorbar(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = imshow(targets[0,:,:,:, 0].sum(axis=-1))\n",
    "plt.colorbar(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = imshow(pois[0,:,:,:, 0].sum(axis=-1))\n",
    "# plt.colorbar(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = imshow(features[0,:,:,:, 0].sum(axis=-1))\n",
    "plt.colorbar(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "module = hub.Module(modulepath + '/likelihood/')\n",
    "\n",
    "# module = hub.Module('../code/models/galmodel/pad2/module/1546484684/likelihood/')\n",
    "\n",
    "xx = tf.placeholder(tf.float32, shape=[None, None, None, None, nchannels], name='input')\n",
    "yy = tf.placeholder(tf.float32, shape=[None, None, None, None, ntarget], name='labels')\n",
    "samples = module(dict(features=xx, labels=yy), as_dict=True)['sample']\n",
    "# loglik = module(dict(features=xx, labels=yy), as_dict=True)['difference']\n",
    "# poisson = module(dict(features=xx, labels=yy), as_dict=True)['lambda']\n",
    "\n",
    "preds = {}\n",
    "pois = {}\n",
    "llik = {}\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initializers.global_variables())\n",
    "\n",
    "    for seed in seeds:\n",
    "        xxm = np.pad(meshes[seed][0]['cic'], pad, 'wrap')\n",
    "        xxm = np.expand_dims(np.expand_dims(xxm,  -1), 0)\n",
    "\n",
    "        yym = np.stack([meshes[seed][1]['pnn']], axis=-1)\n",
    "        yym = np.expand_dims(yym, 0)\n",
    "        preds[seed] = sess.run(samples, feed_dict={xx:xxm, yy:yym})\n",
    "        meshes[seed][0]['predict'] = np.squeeze(preds[seed][:, :, :])\n",
    "#         pois[seed] = sess.run(poisson, feed_dict={xx:xxm, yy:yym})\n",
    "#         meshes[seed][0]['lambda'] = np.squeeze(pois[seed][:, :, :])\n",
    "#         llik[seed] = sess.run(loglik, feed_dict={xx:xxm, yy:yym})\n",
    "#         meshes[seed][0]['loglik'] = np.squeeze(llik[seed][:, :, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "##Power spectrum\n",
    "shape = [nc,nc,nc]\n",
    "kk = tools.fftk(shape, bs)\n",
    "kmesh = sum(i**2 for i in kk)**0.5\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (10, 5))\n",
    "for seed in seeds:\n",
    "    key = ''\n",
    "    predict, hpmeshd = meshes[seed][0]['predict%s'%key] , meshes[seed][1]['pnn%s'%key], \n",
    "#     predict, hpmeshd = meshes[seed][0]['lambda%s'%key] , meshes[seed][1]['pnn%s'%key], \n",
    "#     predict, hpmeshd = meshes[seed][0]['lambda%s'%key] , meshes[seed][0]['cic%s'%key], \n",
    "    k, pkpred = tools.power(predict/predict.mean(), boxsize=bs, k=kmesh)\n",
    "    k, pkhd = tools.power(hpmeshd/hpmeshd.mean(), boxsize=bs, k=kmesh)\n",
    "    k, pkhx = tools.power(hpmeshd/hpmeshd.mean(), predict/predict.mean(), boxsize=bs, k=kmesh)    \n",
    "##\n",
    "    ax[0].semilogx(k[1:], pkpred[1:]/pkhd[1:], label=seed)\n",
    "    ax[1].semilogx(k[1:], pkhx[1:]/(pkpred[1:]*pkhd[1:])**0.5)\n",
    "    \n",
    "for axis in ax.flatten():\n",
    "    axis.legend(fontsize=14)\n",
    "    axis.set_yticks(np.arange(0, 1.1, 0.1))\n",
    "    axis.grid(which='both')\n",
    "    axis.set_ylim(0.,1.1)\n",
    "ax[0].set_ylabel('Transfer function', fontsize=14)\n",
    "ax[1].set_ylabel('Cross correlation', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# ##################################################\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10,5))\n",
    "\n",
    "key = ''\n",
    "predict, hpmeshd = meshes[seed][0]['predict%s'%key] , meshes[seed][1]['pnn%s'%key], \n",
    "# predict, hpmeshd = meshes[seed][0]['lambda%s'%key] , meshes[seed][1]['pnn%s'%key], \n",
    "vmin, vmax = 0, (hpmeshd[:, :, :].sum(axis=0)).max()\n",
    "vmin, vmax =  None, None\n",
    "im = ax[0].imshow(predict[:, :, :].sum(axis=0), vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(im, ax=ax[0])\n",
    "im = ax[1].imshow(hpmeshd[:, :, :].sum(axis=0), vmin=vmin, vmax=vmax)\n",
    "plt.colorbar(im, ax=ax[1])\n",
    "ax[0].set_ylabel('Prediction', fontsize=15)\n",
    "ax[1].set_ylabel('Truth', fontsize=15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
